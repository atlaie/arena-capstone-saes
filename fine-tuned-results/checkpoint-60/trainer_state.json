{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.13745704467353953,
  "eval_steps": 500,
  "global_step": 60,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002290950744558992,
      "grad_norm": 2.085380792617798,
      "learning_rate": 4e-05,
      "loss": 3.3145,
      "step": 1
    },
    {
      "epoch": 0.004581901489117984,
      "grad_norm": 2.5497167110443115,
      "learning_rate": 8e-05,
      "loss": 3.3691,
      "step": 2
    },
    {
      "epoch": 0.006872852233676976,
      "grad_norm": 2.3752660751342773,
      "learning_rate": 0.00012,
      "loss": 3.0754,
      "step": 3
    },
    {
      "epoch": 0.009163802978235968,
      "grad_norm": 2.2313759326934814,
      "learning_rate": 0.00016,
      "loss": 3.0228,
      "step": 4
    },
    {
      "epoch": 0.011454753722794959,
      "grad_norm": 3.188750982284546,
      "learning_rate": 0.0002,
      "loss": 3.4862,
      "step": 5
    },
    {
      "epoch": 0.013745704467353952,
      "grad_norm": 3.207082986831665,
      "learning_rate": 0.00019636363636363636,
      "loss": 3.5994,
      "step": 6
    },
    {
      "epoch": 0.016036655211912942,
      "grad_norm": 2.1332874298095703,
      "learning_rate": 0.00019272727272727274,
      "loss": 2.9631,
      "step": 7
    },
    {
      "epoch": 0.018327605956471937,
      "grad_norm": 2.4442405700683594,
      "learning_rate": 0.0001890909090909091,
      "loss": 2.9206,
      "step": 8
    },
    {
      "epoch": 0.020618556701030927,
      "grad_norm": 1.9670313596725464,
      "learning_rate": 0.00018545454545454545,
      "loss": 2.6741,
      "step": 9
    },
    {
      "epoch": 0.022909507445589918,
      "grad_norm": 2.8226683139801025,
      "learning_rate": 0.00018181818181818183,
      "loss": 3.716,
      "step": 10
    },
    {
      "epoch": 0.025200458190148912,
      "grad_norm": 2.6521503925323486,
      "learning_rate": 0.0001781818181818182,
      "loss": 2.5092,
      "step": 11
    },
    {
      "epoch": 0.027491408934707903,
      "grad_norm": 2.507603645324707,
      "learning_rate": 0.00017454545454545454,
      "loss": 2.4534,
      "step": 12
    },
    {
      "epoch": 0.029782359679266894,
      "grad_norm": 2.784121036529541,
      "learning_rate": 0.0001709090909090909,
      "loss": 2.5428,
      "step": 13
    },
    {
      "epoch": 0.032073310423825885,
      "grad_norm": 2.966508388519287,
      "learning_rate": 0.00016727272727272728,
      "loss": 2.5336,
      "step": 14
    },
    {
      "epoch": 0.03436426116838488,
      "grad_norm": 2.5850448608398438,
      "learning_rate": 0.00016363636363636366,
      "loss": 2.0527,
      "step": 15
    },
    {
      "epoch": 0.03665521191294387,
      "grad_norm": 2.7383556365966797,
      "learning_rate": 0.00016,
      "loss": 2.4323,
      "step": 16
    },
    {
      "epoch": 0.038946162657502864,
      "grad_norm": 2.694305419921875,
      "learning_rate": 0.00015636363636363637,
      "loss": 2.3482,
      "step": 17
    },
    {
      "epoch": 0.041237113402061855,
      "grad_norm": 3.4534332752227783,
      "learning_rate": 0.00015272727272727275,
      "loss": 2.2632,
      "step": 18
    },
    {
      "epoch": 0.043528064146620846,
      "grad_norm": 2.7192039489746094,
      "learning_rate": 0.0001490909090909091,
      "loss": 2.0125,
      "step": 19
    },
    {
      "epoch": 0.045819014891179836,
      "grad_norm": 2.661252975463867,
      "learning_rate": 0.00014545454545454546,
      "loss": 1.9644,
      "step": 20
    },
    {
      "epoch": 0.048109965635738834,
      "grad_norm": 3.1254470348358154,
      "learning_rate": 0.00014181818181818184,
      "loss": 2.0412,
      "step": 21
    },
    {
      "epoch": 0.050400916380297825,
      "grad_norm": 2.9892635345458984,
      "learning_rate": 0.0001381818181818182,
      "loss": 2.1777,
      "step": 22
    },
    {
      "epoch": 0.052691867124856816,
      "grad_norm": 2.5585758686065674,
      "learning_rate": 0.00013454545454545455,
      "loss": 2.0757,
      "step": 23
    },
    {
      "epoch": 0.054982817869415807,
      "grad_norm": 2.6185991764068604,
      "learning_rate": 0.00013090909090909093,
      "loss": 2.1821,
      "step": 24
    },
    {
      "epoch": 0.0572737686139748,
      "grad_norm": 2.567223072052002,
      "learning_rate": 0.00012727272727272728,
      "loss": 2.1299,
      "step": 25
    },
    {
      "epoch": 0.05956471935853379,
      "grad_norm": 2.1453921794891357,
      "learning_rate": 0.00012363636363636364,
      "loss": 1.8326,
      "step": 26
    },
    {
      "epoch": 0.061855670103092786,
      "grad_norm": 2.235825777053833,
      "learning_rate": 0.00012,
      "loss": 2.044,
      "step": 27
    },
    {
      "epoch": 0.06414662084765177,
      "grad_norm": 2.165884494781494,
      "learning_rate": 0.00011636363636363636,
      "loss": 2.0999,
      "step": 28
    },
    {
      "epoch": 0.06643757159221077,
      "grad_norm": 2.4669673442840576,
      "learning_rate": 0.00011272727272727272,
      "loss": 2.3946,
      "step": 29
    },
    {
      "epoch": 0.06872852233676977,
      "grad_norm": 2.5537028312683105,
      "learning_rate": 0.00010909090909090909,
      "loss": 2.01,
      "step": 30
    },
    {
      "epoch": 0.07101947308132875,
      "grad_norm": 2.343963861465454,
      "learning_rate": 0.00010545454545454545,
      "loss": 2.015,
      "step": 31
    },
    {
      "epoch": 0.07331042382588775,
      "grad_norm": 2.381126880645752,
      "learning_rate": 0.00010181818181818181,
      "loss": 1.9843,
      "step": 32
    },
    {
      "epoch": 0.07560137457044673,
      "grad_norm": 2.533607244491577,
      "learning_rate": 9.818181818181818e-05,
      "loss": 2.0977,
      "step": 33
    },
    {
      "epoch": 0.07789232531500573,
      "grad_norm": 2.0974066257476807,
      "learning_rate": 9.454545454545455e-05,
      "loss": 1.8202,
      "step": 34
    },
    {
      "epoch": 0.08018327605956473,
      "grad_norm": 2.272080898284912,
      "learning_rate": 9.090909090909092e-05,
      "loss": 1.7423,
      "step": 35
    },
    {
      "epoch": 0.08247422680412371,
      "grad_norm": 2.7131242752075195,
      "learning_rate": 8.727272727272727e-05,
      "loss": 2.2229,
      "step": 36
    },
    {
      "epoch": 0.08476517754868271,
      "grad_norm": 2.3459904193878174,
      "learning_rate": 8.363636363636364e-05,
      "loss": 2.0352,
      "step": 37
    },
    {
      "epoch": 0.08705612829324169,
      "grad_norm": 2.3788390159606934,
      "learning_rate": 8e-05,
      "loss": 2.009,
      "step": 38
    },
    {
      "epoch": 0.08934707903780069,
      "grad_norm": 2.824880838394165,
      "learning_rate": 7.636363636363637e-05,
      "loss": 2.1384,
      "step": 39
    },
    {
      "epoch": 0.09163802978235967,
      "grad_norm": 2.673635721206665,
      "learning_rate": 7.272727272727273e-05,
      "loss": 2.324,
      "step": 40
    },
    {
      "epoch": 0.09392898052691867,
      "grad_norm": 2.720015048980713,
      "learning_rate": 6.90909090909091e-05,
      "loss": 2.2222,
      "step": 41
    },
    {
      "epoch": 0.09621993127147767,
      "grad_norm": 2.092952013015747,
      "learning_rate": 6.545454545454546e-05,
      "loss": 2.0819,
      "step": 42
    },
    {
      "epoch": 0.09851088201603665,
      "grad_norm": 2.133573055267334,
      "learning_rate": 6.181818181818182e-05,
      "loss": 1.9557,
      "step": 43
    },
    {
      "epoch": 0.10080183276059565,
      "grad_norm": 2.282809019088745,
      "learning_rate": 5.818181818181818e-05,
      "loss": 1.9254,
      "step": 44
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 2.3827402591705322,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 2.04,
      "step": 45
    },
    {
      "epoch": 0.10538373424971363,
      "grad_norm": 1.9673806428909302,
      "learning_rate": 5.090909090909091e-05,
      "loss": 1.9115,
      "step": 46
    },
    {
      "epoch": 0.10767468499427263,
      "grad_norm": 2.2100038528442383,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 1.997,
      "step": 47
    },
    {
      "epoch": 0.10996563573883161,
      "grad_norm": 2.1285765171051025,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 1.9834,
      "step": 48
    },
    {
      "epoch": 0.11225658648339061,
      "grad_norm": 2.2788562774658203,
      "learning_rate": 4e-05,
      "loss": 2.0923,
      "step": 49
    },
    {
      "epoch": 0.1145475372279496,
      "grad_norm": 2.2731804847717285,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 1.692,
      "step": 50
    },
    {
      "epoch": 0.11683848797250859,
      "grad_norm": 2.8171393871307373,
      "learning_rate": 3.272727272727273e-05,
      "loss": 2.4327,
      "step": 51
    },
    {
      "epoch": 0.11912943871706758,
      "grad_norm": 2.400787591934204,
      "learning_rate": 2.909090909090909e-05,
      "loss": 2.2437,
      "step": 52
    },
    {
      "epoch": 0.12142038946162657,
      "grad_norm": 2.09363055229187,
      "learning_rate": 2.5454545454545454e-05,
      "loss": 1.9121,
      "step": 53
    },
    {
      "epoch": 0.12371134020618557,
      "grad_norm": 2.499880075454712,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 2.2312,
      "step": 54
    },
    {
      "epoch": 0.12600229095074456,
      "grad_norm": 2.3710763454437256,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.774,
      "step": 55
    },
    {
      "epoch": 0.12829324169530354,
      "grad_norm": 2.0257227420806885,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 2.0342,
      "step": 56
    },
    {
      "epoch": 0.13058419243986255,
      "grad_norm": 2.099810838699341,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 1.7594,
      "step": 57
    },
    {
      "epoch": 0.13287514318442153,
      "grad_norm": 2.110982894897461,
      "learning_rate": 7.272727272727272e-06,
      "loss": 1.8741,
      "step": 58
    },
    {
      "epoch": 0.13516609392898052,
      "grad_norm": 2.462287187576294,
      "learning_rate": 3.636363636363636e-06,
      "loss": 1.6732,
      "step": 59
    },
    {
      "epoch": 0.13745704467353953,
      "grad_norm": 2.6348419189453125,
      "learning_rate": 0.0,
      "loss": 1.978,
      "step": 60
    }
  ],
  "logging_steps": 1,
  "max_steps": 60,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.217001198419968e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
