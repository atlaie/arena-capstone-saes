AUTOTUNE bmm(8x512x256, 8x256x512)
  bmm 0.1618 ms 100.0%
  triton_bmm_994 0.1649 ms 98.1%
  triton_bmm_998 0.1725 ms 93.8%
  triton_bmm_1001 0.1751 ms 92.4%
  triton_bmm_1002 0.1772 ms 91.3%
  triton_bmm_993 0.1915 ms 84.5%
  triton_bmm_997 0.1966 ms 82.3%
  triton_bmm_1003 0.1997 ms 81.0%
  triton_bmm_990 0.2253 ms 71.8%
  triton_bmm_991 0.2304 ms 70.2%
SingleProcess AUTOTUNE benchmarking takes 2.5408 seconds and 0.0333 seconds precompiling
AUTOTUNE bmm(8x512x512, 8x512x256)
  bmm 0.0891 ms 100.0%
  triton_bmm_1013 0.1853 ms 48.1%
  triton_bmm_1012 0.1894 ms 47.0%
  triton_bmm_1017 0.1966 ms 45.3%
  triton_bmm_1021 0.1966 ms 45.3%
  triton_bmm_1025 0.1966 ms 45.3%
  triton_bmm_1020 0.2089 ms 42.6%
  triton_bmm_1022 0.2171 ms 41.0%
  triton_bmm_1009 0.2263 ms 39.4%
  triton_bmm_1016 0.2304 ms 38.7%
SingleProcess AUTOTUNE benchmarking takes 2.2894 seconds and 0.0014 seconds precompiling
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcf5629fca0, execution_count=1 error_before_exec=None error_in_exec=[enforce fail at inline_container.cc:603] . unexpected pos 3671411648 vs 3671411536 info=<ExecutionInfo object at 7fcf5629ff70, raw_cell="#%%
# ============================================.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X16sdW50aXRsZWQ%3D> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc9d175d80, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fccd83d8c10, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc9d175d80, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fccb4a35ea0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fccb4603490, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fccb4a35ea0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fccb41248e0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fccac239030, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fccb41248e0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc7dca8340, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc7dca8df0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc7dca8340, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc8cc64730, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc7de08370, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc8cc64730, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc7de09600, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc84406ef0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc7de09600, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc84600df0, raw_cell="#%%
model.save_pretrained("./gemma-2-2b-it_noLoRa.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X20sdW50aXRsZWQ%3D>,),kwargs {}:
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc84600820, execution_count=2 error_before_exec=None error_in_exec=
            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'lm_head.weight', 'model.embed_tokens.weight'}].
            A potential way to correctly save your model is to use `save_model`.
            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors
             info=<ExecutionInfo object at 7fcc84600df0, raw_cell="#%%
model.save_pretrained("./gemma-2-2b-it_noLoRa.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X20sdW50aXRsZWQ%3D> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fccb4603490, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc8df46d10, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fccb4603490, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc7da75450, raw_cell="#%%
# Freeze all layers
for param in model.parame.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X21sdW50aXRsZWQ%3D>,),kwargs {}:
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc7da75480, execution_count=3 error_before_exec=None error_in_exec=
            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'lm_head.weight', 'model.embed_tokens.weight'}].
            A potential way to correctly save your model is to use `save_model`.
            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors
             info=<ExecutionInfo object at 7fcc7da75450, raw_cell="#%%
# Freeze all layers
for param in model.parame.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X21sdW50aXRsZWQ%3D> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fccac2380d0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fccac23bbb0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fccac2380d0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc9cd9e7d0, raw_cell="#%%
model.save_pretrained("./gemma-2-2b-it_noLoRa.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X22sdW50aXRsZWQ%3D>,),kwargs {}:
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc84602230, execution_count=4 error_before_exec=None error_in_exec=[enforce fail at inline_container.cc:603] . unexpected pos 38336 vs 38230 info=<ExecutionInfo object at 7fcc9cd9e7d0, raw_cell="#%%
model.save_pretrained("./gemma-2-2b-it_noLoRa.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X22sdW50aXRsZWQ%3D> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc85db19c0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc85db0820, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc85db19c0, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc9d175d80, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc8d41afe0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc9d175d80, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fccb4cfb1f0, raw_cell="#%%
from transformers import AutoModelForCausalLM.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X23sdW50aXRsZWQ%3D>,),kwargs {}:
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fccb4cf8dc0, execution_count=5 error_before_exec=None error_in_exec=Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory gemma-2-2b-it_noLoRa_original-dataset. info=<ExecutionInfo object at 7fccb4cfb1f0, raw_cell="#%%
from transformers import AutoModelForCausalLM.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X23sdW50aXRsZWQ%3D> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc9cbf0520, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoModelForCausalLM", "type": "type", "fullType": "type"}, {"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc8d50e830, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc9cbf0520, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc84771060, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoModelForCausalLM", "type": "type", "fullType": "type"}, {"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc84771750, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc84771060, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc9d175d80, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoModelForCausalLM", "type": "type", "fullType": "type"}, {"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc8d38cfd0, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc9d175d80, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc84347df0, raw_cell="#%%
model.save_pretrained("./gemma-2-2b-it_noLoRa.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X24sdW50aXRsZWQ%3D>,),kwargs {}:
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc84346a10, execution_count=6 error_before_exec=None error_in_exec=[enforce fail at inline_container.cc:603] . unexpected pos 38336 vs 38230 info=<ExecutionInfo object at 7fcc84347df0, raw_cell="#%%
model.save_pretrained("./gemma-2-2b-it_noLoRa.." store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Interactive-2.interactive#X24sdW50aXRsZWQ%3D> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc9c691f30, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoModelForCausalLM", "type": "type", "fullType": "type"}, {"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fcc9c693f70, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fcc9c691f30, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:
Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fcce1521750>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7fcc8cc8c190, raw_cell="def _VSCODE_getVariable(what_to_get, is_debugging,.." store_history=False silent=False shell_futures=True cell_id=None>,),kwargs {}:
[{"name": "AutoModelForCausalLM", "type": "type", "fullType": "type"}, {"name": "AutoTokenizer", "type": "type", "fullType": "type"}, {"name": "DataCollatorForSeq2Seq", "type": "type", "fullType": "type"}, {"name": "DataLoader", "type": "type", "fullType": "type"}, {"name": "Dataset", "type": "type", "fullType": "type"}, {"name": "FastLanguageModel", "type": "type", "fullType": "type"}, {"name": "HookedTransformer", "type": "type", "fullType": "type"}, {"name": "SFTTrainer", "type": "type", "fullType": "type"}, {"name": "TrainingArguments", "type": "type", "fullType": "type"}, {"name": "batch_size", "type": "int", "fullType": "int"}, {"name": "data_collator", "type": "DataCollatorForSeq2Seq", "fullType": "transformers.data.data_collator.DataCollatorForSeq2Seq"}, {"name": "device", "type": "device", "fullType": "torch.device"}, {"name": "ds", "type": "DatasetDict", "fullType": "datasets.dataset_dict.DatasetDict"}, {"name": "dtype", "type": "NoneType", "fullType": "NoneType"}, {"name": "elem", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "get_ipython", "type": "function", "fullType": "function"}, {"name": "ii", "type": "int", "fullType": "int"}, {"name": "is_bfloat16_supported", "type": "function", "fullType": "function"}, {"name": "load_dataset", "type": "function", "fullType": "function"}, {"name": "load_in_4bit", "type": "bool", "fullType": "bool"}, {"name": "max_seq_length", "type": "int", "fullType": "int"}, {"name": "model", "type": "Gemma2ForCausalLM", "fullType": "transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM"}, {"name": "os", "type": "module", "fullType": "module"}, {"name": "param", "type": "Parameter", "fullType": "torch.nn.parameter.Parameter"}, {"name": "pd", "type": "module", "fullType": "module"}, {"name": "test_dataloader", "type": "DataLoader", "fullType": "torch.utils.data.dataloader.DataLoader"}, {"name": "tokenize_function_with_choices_test", "type": "function", "fullType": "function"}, {"name": "tokenized_dataset_test", "type": "Dataset", "fullType": "datasets.arrow_dataset.Dataset"}, {"name": "tokenizer", "type": "GemmaTokenizerFast", "fullType": "transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast"}, {"name": "torch", "type": "module", "fullType": "module"}, {"name": "trainable_parameters", "type": "list", "fullType": "list"}, {"name": "trainer", "type": "SFTTrainer", "fullType": "trl.trainer.sft_trainer.SFTTrainer"}, {"name": "use_bfloat16", "type": "bool", "fullType": "bool"}]
