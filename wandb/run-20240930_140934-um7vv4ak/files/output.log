
It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.
Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f46edee74c0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f45c0818160, execution_count=4 error_before_exec=None error_in_exec=The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask. info=<ExecutionInfo object at 7f45c081bd90, raw_cell="#%%
trainer = Trainer(
    model=model,
